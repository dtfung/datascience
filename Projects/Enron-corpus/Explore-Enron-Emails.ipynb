{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring the Enron Emails Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Enron corpus is the largest public domain database of real e-mails in the world.  This version of the dataset contains over 500,000 emails from about 150 users, mostly senior management at Enron.  The corpus is valuable for research in that it provides a rich example of how a real organization uses e-mails and has had a widespread influence on today's software for fraud detection.  Visit [here](https://en.wikipedia.org/wiki/Enron_scandal) to learn more about the Enron scandal.  \n",
    "\n",
    "The purpose of this project is to explore the data to check for fraud and to see what sort of information was leaked in the e-mails.  Checkout the data set [here](https://www.cs.cmu.edu/~./enron/).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Looking At The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded 517401 rows and 2 columns!\n",
      "Index([u'file', u'message'], dtype='object')\n",
      "                       file                                            message\n",
      "0     allen-p/_sent_mail/1.  Message-ID: <18782981.1075855378110.JavaMail.e...\n",
      "1    allen-p/_sent_mail/10.  Message-ID: <15464986.1075855378456.JavaMail.e...\n",
      "2   allen-p/_sent_mail/100.  Message-ID: <24216240.1075855687451.JavaMail.e...\n",
      "3  allen-p/_sent_mail/1000.  Message-ID: <13505866.1075863688222.JavaMail.e...\n",
      "4  allen-p/_sent_mail/1001.  Message-ID: <30922949.1075863688243.JavaMail.e...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "filepath = \"data/emails.csv\"\n",
    "# Read the data into a pandas dataframe called emails\n",
    "emails = pd.read_csv(filepath)\n",
    "print(\"Successfully loaded {} rows and {} columns!\".format(emails.shape[0], emails.shape[1]))\n",
    "# Print column names\n",
    "print(emails.columns)\n",
    "# Store column headers \n",
    "headers = [header for header in emails.columns]\n",
    "# Print the first 5 rows of the dataset\n",
    "print(emails.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numpy and pandas were imported, then the csv file containing the e-mails was read into a dataframe called **`emails`**.  The reading may take a while due to the size of the file.  Next, the shape of the dataset, column names and a sample of five rows within the dataset were printed.  There are 517,401 rows and 2 columns.  \n",
    "\n",
    "**`file`** - contains the original directory and filename of each email. The root level of this path is the employee (surname first followed by first name initial) to whom the emails belong. \n",
    "\n",
    "**`message`** - contains the email text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E-mails are MIME formatted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a sample of the standard e-mail found in the data.  It contains a list of headers and a message body.  Note that there is a header label called \"Mime-Version\", which signifies that the e-mails in this dataset are MIME formatted.  MIME stands for Multipurpose Internet Mail Extensions and virtually all human-written email is transmitted in MIME format.  Python has a built in [MIME handling package](https://docs.python.org/2/library/email.html) and this is what will be used to dissect the data needed out of each e-mail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message-ID: <18782981.1075855378110.JavaMail.evans@thyme>\n",
      "Date: Mon, 14 May 2001 16:39:00 -0700 (PDT)\n",
      "From: phillip.allen@enron.com\n",
      "To: tim.belden@enron.com\n",
      "Subject: \n",
      "Mime-Version: 1.0\n",
      "Content-Type: text/plain; charset=us-ascii\n",
      "Content-Transfer-Encoding: 7bit\n",
      "X-From: Phillip K Allen\n",
      "X-To: Tim Belden <Tim Belden/Enron@EnronXGate>\n",
      "X-cc: \n",
      "X-bcc: \n",
      "X-Folder: \\Phillip_Allen_Jan2002_1\\Allen, Phillip K.\\'Sent Mail\n",
      "X-Origin: Allen-P\n",
      "X-FileName: pallen (Non-Privileged).pst\n",
      "\n",
      "Here is our forecast\n",
      "\n",
      " \n"
     ]
    }
   ],
   "source": [
    "print(emails.loc[0][\"message\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the list of things that needs to be performed on the data before:\n",
    "* Check for missing values\n",
    "* Tokenization\n",
    "* Feature Engineering\n",
    "* Remove unwanted characters "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `emails` dataframe was checked for missing values.  In this case, there were no missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No NaN values\n"
     ]
    }
   ],
   "source": [
    "# Check for null values\n",
    "null_values = emails.isnull().values.any()\n",
    "if null_values == False: print \"No NaN values\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introducing the Bag-of-words model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the computer to make inferences of the e-mails, it has to be able to interpret the text by making a numerical representation of it.  One way to do this is by using something called a [**Bag-of-words model**](https://en.wikipedia.org/wiki/Bag-of-words_model).  It will take each e-mail as a string and convert it into a numerical vector.  In this case, each string will be converted into a 1-dimensional array of 0s and 1s.  The first step in creating a Bag-of-words model is called tokenization.  By tokenizing each e-mail, each string is split into a list of words. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, the MIME handling python package mentioned earlier was used to extract both the headers and the messages found within each e-mail.  The data found within the headers section of each e-mail will be added to the `emails` dataframe as new features.  They are stored in the `new_features` dictionary.  All tokens are stored in `tokenized_messages` for further processing.\n",
    "\n",
    "**Why lowercase the message body?**\n",
    "\n",
    "Because a human may know that \"Forecast\" and \"forecast\" means the same thing, but the computer does not know this.  Also, while building the matrix using the bag-of-words model, lowercasing also reduces the chance of the same word being duplicated and entered as a separate word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: Running the code below may take a few minutes to complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['here', 'is', 'our', 'forecast\\n\\n', '']\n"
     ]
    }
   ],
   "source": [
    "# MIME handling package\n",
    "import email\n",
    "\n",
    "# List of tokens\n",
    "tokenized_messages = []\n",
    "# Used to store data for new features\n",
    "new_features = {}\n",
    "\n",
    "for item in emails[\"message\"]: \n",
    "    # Return a message object structure from a string\n",
    "    e = email.message_from_string(item)\n",
    "    # A list of tuples containing the header keys and values\n",
    "    header_list = e.items()\n",
    "    # Add data to dictionary \n",
    "    for key, value in header_list:\n",
    "        if key in new_features:\n",
    "            values = new_features.get(key)\n",
    "            values.append(value)\n",
    "            new_features[key] = values\n",
    "        else:\n",
    "            new_features[key] = [value]\n",
    "    # get message body  \n",
    "    message_body = e.get_payload()\n",
    "    # lower case messages\n",
    "    message_body = message_body.lower()\n",
    "    # split message into tokens\n",
    "    tokens = message_body.split(\" \")\n",
    "    tokenized_messages.append(tokens)\n",
    "print(tokenized_messages[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding new columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#create new column for employee name using data from the \"file\" column\n",
    "employees = []\n",
    "\n",
    "# empty array to store shorter and longer than average directories\n",
    "outlier_dir = []\n",
    "# extract \"file\" column\n",
    "file_info = emails[headers[0]]\n",
    "for row in file_info:\n",
    "    tokens = row.split(\"/\")\n",
    "    if len(tokens) < 3 or len(tokens)> 3:\n",
    "        outlier_dir.append(tokens)\n",
    "    employees.append(tokens[0])\n",
    "# create column and set its values\n",
    "emails[\"employee\"] = employees "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       file  \\\n",
      "0     allen-p/_sent_mail/1.   \n",
      "1    allen-p/_sent_mail/10.   \n",
      "2   allen-p/_sent_mail/100.   \n",
      "3  allen-p/_sent_mail/1000.   \n",
      "4  allen-p/_sent_mail/1001.   \n",
      "\n",
      "                                             message employee  \n",
      "0  Message-ID: <18782981.1075855378110.JavaMail.e...  allen-p  \n",
      "1  Message-ID: <15464986.1075855378456.JavaMail.e...  allen-p  \n",
      "2  Message-ID: <24216240.1075855687451.JavaMail.e...  allen-p  \n",
      "3  Message-ID: <13505866.1075863688222.JavaMail.e...  allen-p  \n",
      "4  Message-ID: <30922949.1075863688243.JavaMail.e...  allen-p  \n"
     ]
    }
   ],
   "source": [
    "# shows the dataframe with the appended column\n",
    "print(emails.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kaminski-v      28465\n",
      "dasovich-j      28234\n",
      "kean-s          25351\n",
      "mann-k          23381\n",
      "jones-t         19950\n",
      "shackleton-s    18687\n",
      "taylor-m        13875\n",
      "farmer-d        13032\n",
      "germany-c       12436\n",
      "beck-s          11830\n",
      "Name: employee, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# show the number of emails sent by each employee\n",
    "print(emails[\"employee\"].value_counts()[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26555\n"
     ]
    }
   ],
   "source": [
    "print(len(outlier_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The e-mails could be filtered by employee names, which can be retrived from the filename found in the `file` column.  Although the subfolders in the filepath could be used as filters, they will remain untouched for now.\n",
    "\n",
    "An empty array was created to hold the values for the `employee` column.  The `file` column from the `emails` dataframe was extracted into an array called `file_info`.  `file_info` was then loooped over to get the string found at each index. Each string was then split into tokens, with the employee name located at index 0.  Each name was then added to the `employees` array.  This array was then set to be the values in the new `employee` column.  \n",
    "\n",
    "A sample of five rows was printed to show that the new column was added.  Also, the number of e-mails sent by each employee was printed in the table.\n",
    "\n",
    "Note that there were also 26,555 directories that contained less or more than three folders.  Although this figure may seem large, it represents approximately 5.0% of all directories listed in the dataset.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "###  Remove unwanted HTML Markup, punctuations and emoticons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at a sample e-mail message below and you will see that it contains HTML markup, punctuations, possible emoticons and other unwanted characters.  While it may be useful to retain some punctuations and emoticons, the majority does not contain any useful information for this analysis.  For simplicity, all unwanted characters except for possible characters such as \":)\" will be removed.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Message-ID:', '<18782981.1075855378110.JavaMail.evans@thyme>\\nDate:', 'Mon,', '14', 'May', '2001', '16:39:00', '-0700', '(PDT)\\nFrom:', 'phillip.allen@enron.com\\nTo:', 'tim.belden@enron.com\\nSubject:', '\\nMime-Version:', '1.0\\nContent-Type:', 'text/plain;', 'charset=us-ascii\\nContent-Transfer-Encoding:', '7bit\\nX-From:', 'Phillip', 'K', 'Allen\\nX-To:', 'Tim', 'Belden', '<Tim', 'Belden/Enron@EnronXGate>\\nX-cc:', '\\nX-bcc:', '\\nX-Folder:', '\\\\Phillip_Allen_Jan2002_1\\\\Allen,', 'Phillip', \"K.\\\\'Sent\", 'Mail\\nX-Origin:', 'Allen-P\\nX-FileName:', 'pallen', '(Non-Privileged).pst\\n\\nHere', 'is', 'our', 'forecast\\n\\n', '']\n",
      "\n",
      "[['messageid', 'javamailevansthymedate', 'mon', '', 'may', '', '', '', 'pdtfrom', 'phillipallenenroncomto', 'timbeldenenroncomsubject', 'mimeversion', 'contenttype', 'textplain', 'charset=usasciicontenttransferencoding', 'bitxfrom', 'phillip', 'k', 'allen', 'tim', 'belden', 'tim', 'beldenenronenronxgatexcc', 'xbcc', 'xfolder', 'phillip_allen_jan_allen', 'phillip', 'ksent', 'mailxorigin', 'allenpxfilename', 'pallen', 'nonprivilegedpsthere', 'is', 'our', 'forecast', '']]\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_emails[0])\n",
    "print \"\"\n",
    "unwanted_characters = [\",\", \":\", \";\", \".\", \"'\", '\"', \"â€™\", \"?\", \"/\", \"-\", \"+\", \"&\", \n",
    "                       \"<\", \"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \">\", \"@\", \n",
    "                       \"(\", \")\", '\\\\', \"~\", \"{\", \"}\", \"*\", \"^\", \"\\n\", \"xto\"]\n",
    "\n",
    "cleaned_tokenized_emails = []\n",
    "for item in tokenized_emails[:1]:\n",
    "    tokens = []\n",
    "    for token in item:\n",
    "        token = token.lower()\n",
    "        for punc in unwanted_characters:\n",
    "            token = token.replace(punc, \"\")\n",
    "        tokens.append(token)\n",
    "    cleaned_tokenized_emails.append(tokens)\n",
    "print cleaned_tokenized_emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
