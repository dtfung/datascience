{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring the Enron Emails Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Enron corpus is the largest public domain database of real e-mails in the world.  This version of the dataset contains over 500,000 emails from about 150 users, mostly senior management at Enron.  The corpus is valuable for research in that it provides a rich example of how a real organization uses e-mails and has had a widespread influence on today's software for fraud detection.  Visit [here](https://en.wikipedia.org/wiki/Enron_scandal) to learn more about the Enron scandal.  \n",
    "\n",
    "The purpose of this project is to explore the data to check for fraud and to see what sort of information was leaked in the e-mails.  Checkout the data set [here](https://www.cs.cmu.edu/~./enron/).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Looking At The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded 1000 rows and 2 columns!\n",
      "Index([u'file', u'message'], dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>allen-p/_sent_mail/1.</td>\n",
       "      <td>Message-ID: &lt;18782981.1075855378110.JavaMail.e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>allen-p/_sent_mail/10.</td>\n",
       "      <td>Message-ID: &lt;15464986.1075855378456.JavaMail.e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>allen-p/_sent_mail/100.</td>\n",
       "      <td>Message-ID: &lt;24216240.1075855687451.JavaMail.e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>allen-p/_sent_mail/1000.</td>\n",
       "      <td>Message-ID: &lt;13505866.1075863688222.JavaMail.e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>allen-p/_sent_mail/1001.</td>\n",
       "      <td>Message-ID: &lt;30922949.1075863688243.JavaMail.e...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       file                                            message\n",
       "0     allen-p/_sent_mail/1.  Message-ID: <18782981.1075855378110.JavaMail.e...\n",
       "1    allen-p/_sent_mail/10.  Message-ID: <15464986.1075855378456.JavaMail.e...\n",
       "2   allen-p/_sent_mail/100.  Message-ID: <24216240.1075855687451.JavaMail.e...\n",
       "3  allen-p/_sent_mail/1000.  Message-ID: <13505866.1075863688222.JavaMail.e...\n",
       "4  allen-p/_sent_mail/1001.  Message-ID: <30922949.1075863688243.JavaMail.e..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "\n",
    "filepath = \"data/emails.csv\"\n",
    "# Read the data into a pandas dataframe called emails\n",
    "emails = pd.read_csv(filepath)\n",
    "emails = emails.iloc[:1000] # testing \n",
    "print(\"Successfully loaded {} rows and {} columns!\".format(emails.shape[0], emails.shape[1]))\n",
    "# Print column names\n",
    "print(emails.columns)\n",
    "# Store column headers \n",
    "headers = [header for header in emails.columns]\n",
    "# Print the first 5 rows of the dataset\n",
    "print(display(emails.head()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numpy and pandas were imported, then the csv file containing the e-mails was read into a dataframe called **`emails`**.  The reading may take a while due to the size of the file.  Next, the shape of the dataset, column names and a sample of five rows within the dataset were printed.  There are 517,401 rows and 2 columns.  \n",
    "\n",
    "**`file`** - contains the original directory and filename of each email. The root level of this path is the employee (surname first followed by first name initial) to whom the emails belong. \n",
    "\n",
    "**`message`** - contains the email text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E-mails are MIME formatted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a sample of the standard e-mail found in the data.  It contains a list of headers and a message body.  Note that there is a header label called \"Mime-Version\", which signifies that the e-mails in this dataset are MIME formatted.  MIME stands for Multipurpose Internet Mail Extensions and virtually all human-written email is transmitted in MIME format.  Python has a built in [MIME handling package](https://docs.python.org/2/library/email.html) and this is what will be used to dissect the data needed out of each e-mail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message-ID: <18782981.1075855378110.JavaMail.evans@thyme>\n",
      "Date: Mon, 14 May 2001 16:39:00 -0700 (PDT)\n",
      "From: phillip.allen@enron.com\n",
      "To: tim.belden@enron.com\n",
      "Subject: \n",
      "Mime-Version: 1.0\n",
      "Content-Type: text/plain; charset=us-ascii\n",
      "Content-Transfer-Encoding: 7bit\n",
      "X-From: Phillip K Allen\n",
      "X-To: Tim Belden <Tim Belden/Enron@EnronXGate>\n",
      "X-cc: \n",
      "X-bcc: \n",
      "X-Folder: \\Phillip_Allen_Jan2002_1\\Allen, Phillip K.\\'Sent Mail\n",
      "X-Origin: Allen-P\n",
      "X-FileName: pallen (Non-Privileged).pst\n",
      "\n",
      "Here is our forecast\n",
      "\n",
      " \n"
     ]
    }
   ],
   "source": [
    "print(emails.loc[0][\"message\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the list of things that needs to be performed on the data:\n",
    "* Check for missing values\n",
    "* Tokenization\n",
    "* Feature Engineering\n",
    "* Remove unwanted characters "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `emails` dataframe was checked for missing values.  In this case, there were no missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No NaN values\n"
     ]
    }
   ],
   "source": [
    "# Check for null values\n",
    "null_values = emails.isnull().values.any()\n",
    "if null_values == False: print \"No NaN values\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introducing the Bag-of-words model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the computer to make inferences of the e-mails, it has to be able to interpret the text by making a numerical representation of it.  One way to do this is by using something called a [**Bag-of-words model**](https://en.wikipedia.org/wiki/Bag-of-words_model).  It will take each e-mail as a string and convert it into a numerical vector.  In this case, each string will be converted into a 1-dimensional array of 0s and 1s.  The first step in creating a Bag-of-words model is called tokenization.  By tokenizing each e-mail, each string is split into a list of words. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, the MIME handling python package mentioned earlier was used to extract both the headers and the messages found within each e-mail.  The data found within the headers section of each e-mail will be added to the `emails` dataframe as new features.  They are stored in the `header_data` dictionary.  All tokens are stored in `tokenized_messages` for further processing.\n",
    "\n",
    "**Why lowercase the message body?**\n",
    "\n",
    "Because a human may know that \"Forecast\" and \"forecast\" means the same thing, but the computer does not know this.  Also, while building the matrix using the bag-of-words model, lowercasing also reduces the chance of the same word being duplicated and entered as a separate word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: Running the code below may take a few minutes to complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['here', 'is', 'our', 'forecast\\n\\n', '']\n"
     ]
    }
   ],
   "source": [
    "# MIME handling package\n",
    "import email\n",
    "\n",
    "# List of tokens\n",
    "tokenized_messages = []\n",
    "# Used to store data for new features\n",
    "header_data = {}\n",
    "\n",
    "for item in emails[\"message\"]: \n",
    "    # Return a message object structure from a string\n",
    "    e = email.message_from_string(item)\n",
    "    # A list of tuples containing the header keys and values\n",
    "    header_list = e.items()\n",
    "    # Add data to dictionary \n",
    "    for key, value in header_list:\n",
    "        if key in header_data:\n",
    "            values = header_data.get(key)\n",
    "            values.append(value)\n",
    "            header_data[key] = values\n",
    "        else:\n",
    "            header_data[key] = [value]\n",
    "    # get message body  \n",
    "    message_body = e.get_payload()\n",
    "    # lower case messages\n",
    "    message_body = message_body.lower()\n",
    "    # split message into tokens\n",
    "    tokens = message_body.split(\" \")\n",
    "    tokenized_messages.append(tokens)\n",
    "print(tokenized_messages[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding new columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are a list of keys pulled from the `header_data` dictionary which will be used as labels for the new columns in the `email` dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Cc', 'X-cc', 'From', 'Subject', 'X-Folder', 'Content-Transfer-Encoding', 'X-bcc', 'Bcc', 'To', 'X-Origin', 'X-FileName', 'X-From', 'Date', 'X-To', 'Message-ID', 'Content-Type', 'Mime-Version']\n"
     ]
    }
   ],
   "source": [
    "headers = list(header_data.keys())\n",
    "print(headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def add_column(df, data):\n",
    "    # Return our updated dataframe with the added columns\n",
    "    for key, value in data.items():\n",
    "        df[key] = pd.Series(value)\n",
    "    return df\n",
    "emails = add_column(emails, header_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here is a summary of the new `emails` dataset containing all the new columns.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 19)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>message</th>\n",
       "      <th>Cc</th>\n",
       "      <th>X-cc</th>\n",
       "      <th>From</th>\n",
       "      <th>Subject</th>\n",
       "      <th>X-Folder</th>\n",
       "      <th>Content-Transfer-Encoding</th>\n",
       "      <th>X-bcc</th>\n",
       "      <th>Bcc</th>\n",
       "      <th>To</th>\n",
       "      <th>X-Origin</th>\n",
       "      <th>X-FileName</th>\n",
       "      <th>X-From</th>\n",
       "      <th>Date</th>\n",
       "      <th>X-To</th>\n",
       "      <th>Message-ID</th>\n",
       "      <th>Content-Type</th>\n",
       "      <th>Mime-Version</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>allen-p/_sent_mail/1.</td>\n",
       "      <td>Message-ID: &lt;18782981.1075855378110.JavaMail.e...</td>\n",
       "      <td>john.lavorato@enron.com, hunter.shively@enron.com</td>\n",
       "      <td></td>\n",
       "      <td>phillip.allen@enron.com</td>\n",
       "      <td></td>\n",
       "      <td>\\Phillip_Allen_Jan2002_1\\Allen, Phillip K.\\'Se...</td>\n",
       "      <td>7bit</td>\n",
       "      <td></td>\n",
       "      <td>john.lavorato@enron.com, hunter.shively@enron.com</td>\n",
       "      <td>tim.belden@enron.com</td>\n",
       "      <td>Allen-P</td>\n",
       "      <td>pallen (Non-Privileged).pst</td>\n",
       "      <td>Phillip K Allen</td>\n",
       "      <td>Mon, 14 May 2001 16:39:00 -0700 (PDT)</td>\n",
       "      <td>Tim Belden &lt;Tim Belden/Enron@EnronXGate&gt;</td>\n",
       "      <td>&lt;18782981.1075855378110.JavaMail.evans@thyme&gt;</td>\n",
       "      <td>text/plain; charset=us-ascii</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    file                                            message  \\\n",
       "0  allen-p/_sent_mail/1.  Message-ID: <18782981.1075855378110.JavaMail.e...   \n",
       "\n",
       "                                                  Cc X-cc  \\\n",
       "0  john.lavorato@enron.com, hunter.shively@enron.com        \n",
       "\n",
       "                      From Subject  \\\n",
       "0  phillip.allen@enron.com           \n",
       "\n",
       "                                            X-Folder  \\\n",
       "0  \\Phillip_Allen_Jan2002_1\\Allen, Phillip K.\\'Se...   \n",
       "\n",
       "  Content-Transfer-Encoding X-bcc  \\\n",
       "0                      7bit         \n",
       "\n",
       "                                                 Bcc                    To  \\\n",
       "0  john.lavorato@enron.com, hunter.shively@enron.com  tim.belden@enron.com   \n",
       "\n",
       "  X-Origin                   X-FileName           X-From  \\\n",
       "0  Allen-P  pallen (Non-Privileged).pst  Phillip K Allen   \n",
       "\n",
       "                                    Date  \\\n",
       "0  Mon, 14 May 2001 16:39:00 -0700 (PDT)   \n",
       "\n",
       "                                       X-To  \\\n",
       "0  Tim Belden <Tim Belden/Enron@EnronXGate>   \n",
       "\n",
       "                                      Message-ID  \\\n",
       "0  <18782981.1075855378110.JavaMail.evans@thyme>   \n",
       "\n",
       "                   Content-Type Mime-Version  \n",
       "0  text/plain; charset=us-ascii          1.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(emails.shape)\n",
    "print(display(emails.head(1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "###  Remove unwanted HTML Markup, punctuations and emoticons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['here', 'is', 'our', 'forecast  ', '']\n"
     ]
    }
   ],
   "source": [
    "unwanted_characters = [\",\", \":\", \";\", \".\", \"'\", '\"', \"â€™\", \"?\", \"/\", \"-\", \"+\", \"&\", \n",
    "                       \"<\", \"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \">\", \"@\", \n",
    "                       \"(\", \")\", '\\\\', \"~\", \"{\", \"}\", \"*\", \"^\", \"!\", \"\\n\"]\n",
    "\n",
    "cleaned_tokenized_emails = []\n",
    "for item in tokenized_messages:\n",
    "    tokens = []\n",
    "    for token in item:\n",
    "        for punc in unwanted_characters:\n",
    "            token = token.replace(punc, \" \")\n",
    "        tokens.append(token)\n",
    "    cleaned_tokenized_emails.append(tokens)\n",
    "print cleaned_tokenized_emails[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 3. Construct a Bag-of-words model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the data has been cleaned, it is time to construct a bag-of-words model to get the word counts.  Scikit-learn has a `CountVectorizer` class that is able to do just that.  It takes in a list of strings, in our case words, and outputs a dictionary mapping words as keys to their respective integer indices.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count = CountVectorizer()\n",
    "words = ', '.join([str(x) for x in cleaned_tokenized_emails])\n",
    "docs = np.array([words])\n",
    "bag = count.fit_transform(docs)\n",
    "print(count.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we convert the dictionary to a feature vector, where each index position corresponds to the values found in the CountVectorizer vocabulary.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 8058)\n",
      "[[2 1 3 ..., 2 4 2]]\n"
     ]
    }
   ],
   "source": [
    "bag = bag.toarray()\n",
    "print(bag.shape)\n",
    "print(bag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Relevance using term frequency-inverse document frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the Term frequency-inverse document frequency (tf-df) to downweight words that appear frequently in the e-mails but do not contain useful information.  Sci-kit learn has a transformer called the `TfidTransformer` to do this.  The `TfidTransformer` also normalizes the tf-idfs using L2-normalization.  Using L2-normalization helps to penalize the weight of the tf-dfs and prevent overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  1.75562649e-04   8.77813243e-05   2.63343973e-04 ...,   1.75562649e-04\n",
      "    3.51125297e-04   1.75562649e-04]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "tfidf = TfidfTransformer(use_idf=True, norm='l2', smooth_idf=True)\n",
    "print(tfidf.fit_transform(count.fit_transform(docs)).toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
