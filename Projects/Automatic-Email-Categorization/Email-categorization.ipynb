{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic E-mail Categorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The focus of this project is to classify e-mails into folders using the content of each e-mail.  How strong is the correlation between the words in an e-mail and the folder that a user places the e-mail?  To answer these questions, the processed data will be trained, tested and evaluated using several machine learning models.  \n",
    "\n",
    "Undoubtedly, categorizing e-mails based on the content alone can pose so many challenges.  In reality, there is a constant stream of new information being passed through e-mails each day and much of what we learned through previous e-mails may not contain this new information.\n",
    "Also, the content of e-mail threads can branch off onto new topics.  Users could also create mostly unused folders.  The problems go on, but you get the idea of the scope of this problem.  \n",
    "\n",
    "Nonetheless, I found this to be an interesting angle to tackle the Enron Corpus, the largest public domain database in the world containing real-world email messages.  The version used contains over 500,000 emails from about 150 users, mostly senior management at Enron. You can visit [here](https://en.wikipedia.org/wiki/Enron_scandal) to learn more about the Enron scandal.  You can get a copy of the dataset [here](https://www.cs.cmu.edu/~./enron/).  \n",
    "\n",
    "Benchmark results and for this project can be found [here](http://scholarworks.umass.edu/cgi/viewcontent.cgi?article=1217&context=cs_faculty_pubs).  Bekkerman et al. (Ron Bekkerman, Andrew McCallum and Gary Huang). *Automatic Categorization of Email into Folders: Benchmark Experiments on Enron and SRI Corpora*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Exploring the Enron Emails Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded 100 rows and 2 columns!\n",
      "Index([u'file', u'message'], dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>allen-p/_sent_mail/1.</td>\n",
       "      <td>Message-ID: &lt;18782981.1075855378110.JavaMail.e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>allen-p/_sent_mail/10.</td>\n",
       "      <td>Message-ID: &lt;15464986.1075855378456.JavaMail.e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>allen-p/_sent_mail/100.</td>\n",
       "      <td>Message-ID: &lt;24216240.1075855687451.JavaMail.e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>allen-p/_sent_mail/1000.</td>\n",
       "      <td>Message-ID: &lt;13505866.1075863688222.JavaMail.e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>allen-p/_sent_mail/1001.</td>\n",
       "      <td>Message-ID: &lt;30922949.1075863688243.JavaMail.e...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       file                                            message\n",
       "0     allen-p/_sent_mail/1.  Message-ID: <18782981.1075855378110.JavaMail.e...\n",
       "1    allen-p/_sent_mail/10.  Message-ID: <15464986.1075855378456.JavaMail.e...\n",
       "2   allen-p/_sent_mail/100.  Message-ID: <24216240.1075855687451.JavaMail.e...\n",
       "3  allen-p/_sent_mail/1000.  Message-ID: <13505866.1075863688222.JavaMail.e...\n",
       "4  allen-p/_sent_mail/1001.  Message-ID: <30922949.1075863688243.JavaMail.e..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "import email\n",
    "\n",
    "filepath = \"data/emails.csv\"\n",
    "# Read the data into a pandas dataframe called emails\n",
    "emails = pd.read_csv(filepath)\n",
    "emails = emails.iloc[:100] # testing \n",
    "print(\"Successfully loaded {} rows and {} columns!\".format(emails.shape[0], emails.shape[1]))\n",
    "# Print column names\n",
    "print(emails.columns)\n",
    "# Store column headers \n",
    "headers = [header for header in emails.columns]\n",
    "# Print the first 5 rows of the dataset\n",
    "print(display(emails.head()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numpy and pandas were imported, then the csv file containing the e-mails was read into a dataframe called **`emails`**.  The reading may take a while due to the size of the file.  Next, the shape of the dataset, column names and a sample of five rows within the dataset were printed.  There are 517,401 rows and 2 columns.  \n",
    "\n",
    "**`file`** - contains the original directory and filename of each email. The root level of this path is the employee (surname first followed by first name initial) to whom the emails belong. \n",
    "\n",
    "**`message`** - contains the email text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A sample e-mail"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a sample of the standard e-mail found in the data.  It contains a list of headers and a message body.  Note that there is a header label called \"Mime-Version\", which signifies that the e-mails in this dataset are MIME formatted.  MIME stands for Multipurpose Internet Mail Extensions and virtually all human-written email is transmitted in MIME format.  Python has a built in [MIME handling package](https://docs.python.org/2/library/email.html) and this is what will be used to dissect the data needed out of each e-mail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message-ID: <18782981.1075855378110.JavaMail.evans@thyme>\n",
      "Date: Mon, 14 May 2001 16:39:00 -0700 (PDT)\n",
      "From: phillip.allen@enron.com\n",
      "To: tim.belden@enron.com\n",
      "Subject: \n",
      "Mime-Version: 1.0\n",
      "Content-Type: text/plain; charset=us-ascii\n",
      "Content-Transfer-Encoding: 7bit\n",
      "X-From: Phillip K Allen\n",
      "X-To: Tim Belden <Tim Belden/Enron@EnronXGate>\n",
      "X-cc: \n",
      "X-bcc: \n",
      "X-Folder: \\Phillip_Allen_Jan2002_1\\Allen, Phillip K.\\'Sent Mail\n",
      "X-Origin: Allen-P\n",
      "X-FileName: pallen (Non-Privileged).pst\n",
      "\n",
      "Here is our forecast\n",
      "\n",
      " \n",
      "allen-p/_sent_mail/1.\n"
     ]
    }
   ],
   "source": [
    "print(emails.loc[0][\"message\"])\n",
    "print(\"From emails.loc[0][\"file\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the list of things that needs to be performed on the data:\n",
    "* Add new columns in the `emails` dataframe \n",
    "* Check for missing values "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add new columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform e-mail categorization by folder, we will need to split the values found in the \"file\" column to retrieve the employee name and directory.  The values will be added to the `emails` dataframe as new columns.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([u'file', u'message'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print emails.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def insert_value(dictionary, key, value):\n",
    "    if key in dictionary:\n",
    "        values = dictionary.get(key)\n",
    "        values.append(value)\n",
    "        dictionary[key] = values\n",
    "    else:\n",
    "        dictionary[key] = [value]\n",
    "    return dictionary\n",
    "\n",
    "def add_columns(df, column, labels):\n",
    "    new_data = {}\n",
    "    for item in df[column]: \n",
    "        # Split string into tokens\n",
    "        tokens = item.split('/')\n",
    "            # Get employee name (eg.\"allen-p\")\n",
    "        for i in range(0, 2):\n",
    "            value = tokens[i]\n",
    "            key = labels[i]\n",
    "            new_data = insert_value(new_data, key, value)\n",
    "            # Get folder (eg.\"_sent_mail\")         \n",
    "    for key, value in new_data.items():\n",
    "        df[key] = pd.Series(value)\n",
    "    return df\n",
    "emails = add_columns(df = emails, column = \"file\", labels = [\"employee\", \"folder\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here is a summary of the new `emails` dataset containing all the new columns.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>message</th>\n",
       "      <th>employee</th>\n",
       "      <th>folder</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>allen-p/_sent_mail/1.</td>\n",
       "      <td>Message-ID: &lt;18782981.1075855378110.JavaMail.e...</td>\n",
       "      <td>allen-p</td>\n",
       "      <td>_sent_mail</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>allen-p/_sent_mail/10.</td>\n",
       "      <td>Message-ID: &lt;15464986.1075855378456.JavaMail.e...</td>\n",
       "      <td>allen-p</td>\n",
       "      <td>_sent_mail</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>allen-p/_sent_mail/100.</td>\n",
       "      <td>Message-ID: &lt;24216240.1075855687451.JavaMail.e...</td>\n",
       "      <td>allen-p</td>\n",
       "      <td>_sent_mail</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>allen-p/_sent_mail/1000.</td>\n",
       "      <td>Message-ID: &lt;13505866.1075863688222.JavaMail.e...</td>\n",
       "      <td>allen-p</td>\n",
       "      <td>_sent_mail</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>allen-p/_sent_mail/1001.</td>\n",
       "      <td>Message-ID: &lt;30922949.1075863688243.JavaMail.e...</td>\n",
       "      <td>allen-p</td>\n",
       "      <td>_sent_mail</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       file  \\\n",
       "0     allen-p/_sent_mail/1.   \n",
       "1    allen-p/_sent_mail/10.   \n",
       "2   allen-p/_sent_mail/100.   \n",
       "3  allen-p/_sent_mail/1000.   \n",
       "4  allen-p/_sent_mail/1001.   \n",
       "\n",
       "                                             message employee      folder  \n",
       "0  Message-ID: <18782981.1075855378110.JavaMail.e...  allen-p  _sent_mail  \n",
       "1  Message-ID: <15464986.1075855378456.JavaMail.e...  allen-p  _sent_mail  \n",
       "2  Message-ID: <24216240.1075855687451.JavaMail.e...  allen-p  _sent_mail  \n",
       "3  Message-ID: <13505866.1075863688222.JavaMail.e...  allen-p  _sent_mail  \n",
       "4  Message-ID: <30922949.1075863688243.JavaMail.e...  allen-p  _sent_mail  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(emails.shape)\n",
    "print(display(emails.head()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `emails` dataframe was checked for missing values.  In this case, there were no missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No NaN values\n"
     ]
    }
   ],
   "source": [
    "# Check for null values\n",
    "null_values = emails.isnull().values.any()\n",
    "if null_values == False: \n",
    "    print \"No NaN values\"\n",
    "else:\n",
    "    print null_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introducing the Bag-of-words model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the computer to make inferences of the e-mails, it has to be able to interpret the text by making a numerical representation of it.  One way to do this is by using something called a [**Bag-of-words model**](https://en.wikipedia.org/wiki/Bag-of-words_model).  It will take the e-mails as a string and convert it into a numerical vector to show the frequency that each unique word appears over the entire dataset.  The first step in creating a Bag-of-words model is called tokenization.  By tokenizing each e-mail, each string is split into a list of words. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, the MIME handling python package mentioned earlier was used to extract both the headers and the messages found within each e-mail.  The data found within the headers section of each e-mail will be added to the `emails` dataframe as new features.  They are stored in the `header_data` dictionary.  All tokens are stored in `tokenized_messages` for further processing.\n",
    "\n",
    "**Why lowercase the message body?**\n",
    "\n",
    "Because a human may know that \"Forecast\" and \"forecast\" means the same thing, but the computer does not know this.  Also, while building the matrix using the bag-of-words model, lowercasing also reduces the chance of the same word being duplicated and entered as a separate word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Returns a list of tokens\n",
    "def tokenize(df, column):\n",
    "    tokenized_messages = []\n",
    "    for item in df[column]:\n",
    "        # Return a message object structure from a string\n",
    "        e = email.message_from_string(item)    \n",
    "        # get message body  \n",
    "        message_body = e.get_payload()\n",
    "        # lower case messages\n",
    "        message_body = message_body.lower()\n",
    "        # split message into tokens\n",
    "        tokens = message_body.split(\" \")\n",
    "        tokenized_messages.append(tokens)\n",
    "    return tokenized_messages\n",
    "\n",
    "#tokenized_messages = tokenize(emails, \"message\")\n",
    "\n",
    "# Returns a list of tokens\n",
    "def get_messages(df, column):\n",
    "    messages = []\n",
    "    for item in df[column]:\n",
    "        # Return a message object structure from a string\n",
    "        e = email.message_from_string(item)    \n",
    "        # get message body  \n",
    "        message_body = e.get_payload()\n",
    "        # lower case messages\n",
    "        message_body = message_body.lower()\n",
    "        # split message into tokens\n",
    "        #tokens = message_body.split(\" \")\n",
    "        messages.append(message_body)\n",
    "    return messages\n",
    "\n",
    "messages = get_messages(emails, \"message\")\n",
    "print messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "###  Remove unwanted HTML Markup, punctuations and emoticons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "unwanted_characters = [\",\", \":\", \";\", \".\", \"'\", '\"', \"â€™\", \"?\", \"/\", \"-\", \"+\", \"&\", \n",
    "                       \"<\", \"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \">\", \"@\", \n",
    "                       \"(\", \")\", '\\\\', \"~\", \"{\", \"}\", \"*\", \"^\", \"!\", \"\\n\"]\n",
    "\n",
    "import re\n",
    "def remove_special_chars(data):\n",
    "    cleaned_tokenized_emails = []\n",
    "    for item in tokenized_messages:\n",
    "        tokens = []\n",
    "        for token in item:\n",
    "            text = re.sub(r'\\W', \"\", token)\n",
    "            tokens.append(text)\n",
    "        cleaned_tokenized_emails.append(tokens)\n",
    "    return cleaned_tokenized_emails\n",
    "\n",
    "def remove_special_chars2(data):\n",
    "    cleaned_msg = []\n",
    "    for item in data:\n",
    "        text = re.sub(r'\\W', \" \", item)\n",
    "        cleaned_msg.append(text)\n",
    "    return cleaned_msg\n",
    "cleaned_msg = remove_special_chars2(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print cleaned_msg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 3. Construct a Bag-of-words model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the data has been cleaned, it is time to construct a bag-of-words model to get the word counts.  Scikit-learn has a `CountVectorizer` class that is able to do just that.  It takes in a list of strings, in our case words, and outputs a dictionary mapping words as keys to their respective integer indices.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count = CountVectorizer()\n",
    "docs = np.array(cleaned_msg)\n",
    "bag = count.fit_transform(docs)\n",
    "#print(count.vocabulary_)\n",
    "vocab = count.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we convert the dictionary to a feature vector, where each index position corresponds to the values found in the CountVectorizer vocabulary.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2694\n",
      "[0 0 0 ..., 0 0 0]\n",
      "\n",
      "[0 0 0 ..., 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "#print vocab\n",
    "bag = bag.toarray()\n",
    "print(bag[0][1076])\n",
    "print len(bag[0])\n",
    "print(bag[1])\n",
    "print \"\"\n",
    "print(bag[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Relevance using term frequency-inverse document frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the Term frequency-inverse document frequency (tf-df) to downweight words that appear frequently in the e-mails but do not contain useful information.  Sci-kit learn has a transformer called the `TfidTransformer` to do this.  The `TfidTransformer` also normalizes the tf-idfs using L2-normalization.  Using L2-normalization helps to penalize the weight of the tf-dfs and prevent overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "tfidf = TfidfTransformer(use_idf=True, norm='l2', smooth_idf=True)\n",
    "bag = tfidf.fit_transform(count.fit_transform(docs)).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing most relevant words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          frequency\n",
      "capscopyright          9.368033e-07\n",
      "chudson                8.431230e-06\n",
      "cancelledthankssandra  9.368033e-07\n",
      "imageimagecompaq       1.873607e-06\n",
      "tradesten              4.684017e-07\n"
     ]
    }
   ],
   "source": [
    "frequency = []\n",
    "keys = []\n",
    "\n",
    "for key, value in vocab.items():\n",
    "    tally = bag[0][value]\n",
    "    frequency.append(tally)\n",
    "    keys.append(key)\n",
    "\n",
    "words = pd.DataFrame(data = frequency, index = keys, columns = [\"frequency\"])\n",
    "print words.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
